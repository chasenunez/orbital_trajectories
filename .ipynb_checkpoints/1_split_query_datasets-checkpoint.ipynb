{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.14/site-packages (2.4.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting astropy\n",
      "  Downloading astropy-7.2.0-cp311-abi3-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy)\n",
      "  Using cached pyerfa-2.0.1.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (5.7 kB)\n",
      "Collecting astropy-iers-data>=0.2025.10.27.0.39.10 (from astropy)\n",
      "  Using cached astropy_iers_data-0.2026.1.12.0.42.13-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages (from astropy) (6.0.3)\n",
      "Requirement already satisfied: packaging>=22.0.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages (from astropy) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astropy-7.2.0-cp311-abi3-macosx_11_0_arm64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached astropy_iers_data-0.2026.1.12.0.42.13-py3-none-any.whl (2.0 MB)\n",
      "Using cached pyerfa-2.0.1.5-cp39-abi3-macosx_11_0_arm64.whl (329 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, pyerfa, astropy-iers-data, pandas, astropy\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [astropy]━━━\u001b[0m \u001b[32m4/5\u001b[0m [astropy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed astropy-7.2.0 astropy-iers-data-0.2026.1.12.0.42.13 pandas-2.3.3 pyerfa-2.0.1.5 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (only needed once)\n",
    "%pip install numpy pandas astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.time import Time, TimeDelta\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(x0, y0, x1, y1):\n",
    "    ''' Calculate the angle from horizontal, counterclockwise '''\n",
    "    angle = np.rad2deg(np.arctan2(y1-y0, x1-x0))\n",
    "    return angle\n",
    "\n",
    "def split_asteroids(readname, savename, min_distance, max_distance, \n",
    "                    min_diameter, max_diameter, start_date, stop_date, \n",
    "                    randomize=False):\n",
    "    print(savename.split('/')[-1])\n",
    "    df = pd.read_csv(readname, low_memory=False)\n",
    "    print('Original dataset contains', len(df), 'items')\n",
    "\n",
    "    # SPLIT BY DISTANCE FROM SUN, AU\n",
    "    df['q'] = pd.to_numeric(df['q'])\n",
    "    df = df[df['q'] < max_distance]\n",
    "    df = df[df['q'] >= min_distance]\n",
    "    print('Dataset now contains', len(df), 'items', min_distance, '~', \n",
    "          max_distance, 'AU from the sun')\n",
    "\n",
    "    # SPLIT BY DIAMETER, KM\n",
    "    if min_diameter != 'null':\n",
    "        df = df[df['diameter'].astype(float) >= min_diameter]\n",
    "        df = df[df['diameter'].astype(float) < max_diameter]\n",
    "    else:\n",
    "        df = df[pd.isnull(df['diameter'])]\n",
    "        exclude = ['GRK', 'TJN', 'MBA']\n",
    "        df = df[~df['class'].isin(exclude)]\n",
    "        \n",
    "    print('Dataset now contains', len(df), 'items', min_diameter, '~', \n",
    "          max_diameter, 'km in diameter') \n",
    "    \n",
    "    assert df['spkid'].isna().sum() == 0\n",
    "    df['horizons'] = \"DES=+\"+df['spkid'].astype(str)\n",
    "    \n",
    "    # REMOVE DUPLICATES\n",
    "    count = len(df)\n",
    "    df = df.drop_duplicates(keep='first', subset='spkid')\n",
    "    print('Dropped', count-len(df), 'duplicated rows by spkid')\n",
    "    print(len(df[df.duplicated('horizons') == True]), \n",
    "          'duplicated rows remaining by horizons')\n",
    "    \n",
    "    # ADD DATETIME LIMITS\n",
    "    print('Dropped', len(df[df['per'].isna()]), \"NaN values in period data\")\n",
    "    df = df[np.isfinite(df['per'])]\n",
    "    \n",
    "    if randomize != False: \n",
    "        df_named = df[~pd.isnull(df['name'])].copy()\n",
    "        df_notnamed = df[pd.isnull(df['name'])].copy()\n",
    "        if len(df_named) < randomize:\n",
    "            df_sample = df_notnamed.sample(n=randomize-len(df_named))\n",
    "            df = pd.concat([df_sample, df_named])\n",
    "        else:\n",
    "            df = df_named.sample(n=randomize)\n",
    "        assert len(df) == randomize\n",
    "        print(len(df_named), 'named asteroids included in randomized set')\n",
    "    \n",
    "    df1 = df[df['per'] < 40*365].copy()\n",
    "    df1['timedelta'] = TimeDelta(df1['per']*0.25*24*60*60, format='sec')\n",
    "    df1['begin_time'] = Time(Time(start_date, format=\"iso\") - df1['timedelta']).value\n",
    "    df1.drop('timedelta', axis=1, inplace=True)\n",
    "    print(len(df1), 'values truncated because orbital period is shorter than 40 years')\n",
    "    \n",
    "    df2 = df[df['per'] >= 40*365].copy()\n",
    "    df2['begin_time'] = Time(stop_date, format=\"iso\").value\n",
    "    df = pd.concat([df1, df2])\n",
    "    df['end_time'] = Time(start_date, format=\"iso\").value\n",
    "    \n",
    "    df['end_time'] = pd.to_datetime(df['end_time'], dayfirst=False).dt.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    df['begin_time'] = pd.to_datetime(df['begin_time'], dayfirst=False).dt.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    print('Dataset has', len(df), 'items total,', \n",
    "          len(df[df['name'] != np.nan]), 'with proper names.')\n",
    "    \n",
    "    if not os.path.isfile(savename):\n",
    "        df.to_csv(savename, index=False)\n",
    "    else:\n",
    "        print('---NOT SAVED BECAUSE FILE ALREADY EXISTS---\\n')\n",
    "    \n",
    "def split_planets(readname, savename, start_date, stop_date):\n",
    "    \n",
    "    print(savename.split('/')[-1])\n",
    "    df = pd.read_csv(readname, low_memory=False)\n",
    "    print('Original dataset contains', len(df), 'items')\n",
    "\n",
    "    # ADD DATETIME LIMITS\n",
    "    print('Dropped', len(df[df['per'].isna()]), \"NaN values in period data\")\n",
    "    df = df[np.isfinite(df['per'])]\n",
    "    \n",
    "    df['timedelta'] = TimeDelta(df['per']*1*24*60*60, format='sec')\n",
    "    df['begin_time'] = Time(Time(start_date, format=\"iso\") - df['timedelta']).value\n",
    "    df['end_time'] = Time(start_date, format=\"iso\").value\n",
    "    df.drop('timedelta', axis=1, inplace=True)\n",
    "    \n",
    "    df['end_time'] = pd.to_datetime(df['end_time'], dayfirst=False).dt.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    df['begin_time'] = pd.to_datetime(df['begin_time'], dayfirst=False).dt.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            \n",
    "    # DUPLICATES\n",
    "    print(len(df[df.duplicated('horizons') == True]), 'duplicated rows remaining by horizons')\n",
    "    print('Dataset has', len(df), 'items total,', \n",
    "          len(df[df['name'] != np.nan]), 'with proper names.')\n",
    "    \n",
    "    if not os.path.isfile(savename):\n",
    "        df.to_csv(savename, index=False)\n",
    "    else:\n",
    "        print('---NOT SAVED BECAUSE FILE ALREADY EXISTS---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planets.csv\n",
      "Original dataset contains 8 items\n",
      "Dropped 0 NaN values in period data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages/erfa/core.py:133: ErfaWarning: ERFA function \"taiutc\" yielded 1 of \"dubious year (Note 4)\"\n",
      "  warn(f'ERFA function \"{func_name}\" yielded {wmsg}', ErfaWarning)\n",
      "/opt/homebrew/Cellar/jupyterlab/4.5.0_2/libexec/lib/python3.14/site-packages/erfa/core.py:133: ErfaWarning: ERFA function \"d2dtf\" yielded 2 of \"dubious year (Note 5)\"\n",
      "  warn(f'ERFA function \"{func_name}\" yielded {wmsg}', ErfaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicated rows remaining by horizons\n",
      "Dataset has 8 items total, 8 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n",
      "large_asteroids.csv\n",
      "Original dataset contains 794562 items\n",
      "Dataset now contains 794562 items 0 ~ 240 AU from the sun\n",
      "Dataset now contains 2714 items 20 ~ inf km in diameter\n",
      "Dropped 0 duplicated rows by spkid\n",
      "0 duplicated rows remaining by horizons\n",
      "Dropped 0 NaN values in period data\n",
      "2648 values truncated because orbital period is shorter than 40 years\n",
      "Dataset has 2714 items total, 2714 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n",
      "small_asteroids.csv\n",
      "Original dataset contains 794562 items\n",
      "Dataset now contains 794562 items 0 ~ 240 AU from the sun\n",
      "Dataset now contains 7468 items 10 ~ 20 km in diameter\n",
      "Dropped 0 duplicated rows by spkid\n",
      "0 duplicated rows remaining by horizons\n",
      "Dropped 0 NaN values in period data\n",
      "7464 values truncated because orbital period is shorter than 40 years\n",
      "Dataset has 7468 items total, 7468 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n",
      "large_comets.csv\n",
      "Original dataset contains 3568 items\n",
      "Dataset now contains 3568 items 0 ~ 240 AU from the sun\n",
      "Dataset now contains 12 items 10 ~ inf km in diameter\n",
      "Dropped 0 duplicated rows by spkid\n",
      "0 duplicated rows remaining by horizons\n",
      "Dropped 0 NaN values in period data\n",
      "5 values truncated because orbital period is shorter than 40 years\n",
      "Dataset has 12 items total, 12 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n",
      "any_outer_asteroids.csv\n",
      "Original dataset contains 794562 items\n",
      "Dataset now contains 27258 items 3 ~ 240 AU from the sun\n",
      "Dataset now contains 9199 items null ~ null km in diameter\n",
      "Dropped 0 duplicated rows by spkid\n",
      "0 duplicated rows remaining by horizons\n",
      "Dropped 0 NaN values in period data\n",
      "78 named asteroids included in randomized set\n",
      "3129 values truncated because orbital period is shorter than 40 years\n",
      "Dataset has 5000 items total, 5000 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n",
      "any_inner_asteroids.csv\n",
      "Original dataset contains 794562 items\n",
      "Dataset now contains 554983 items 0 ~ 2.5 AU from the sun\n",
      "Dataset now contains 53819 items null ~ null km in diameter\n",
      "Dropped 0 duplicated rows by spkid\n",
      "0 duplicated rows remaining by horizons\n",
      "Dropped 1 NaN values in period data\n",
      "370 named asteroids included in randomized set\n",
      "2996 values truncated because orbital period is shorter than 40 years\n",
      "Dataset has 3000 items total, 3000 with proper names.\n",
      "---NOT SAVED BECAUSE FILE ALREADY EXISTS---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Select asteroids by size and distance from the sun \n",
    "Note: objects are selected by perihelion distance,\n",
    "so they may not be in visible range after getting the exact \n",
    "orbital locations from HORIZONS.\n",
    "'''\n",
    "start_date = '2000-01-01 00:00:00'\n",
    "stop_date = '1990-01-01 00:00:00'\n",
    "\n",
    "# PLANETS\n",
    "readname = './data/planets.csv'\n",
    "savename = './data/planets.csv'\n",
    "split_planets(readname, savename, start_date, stop_date)\n",
    "\n",
    "# ASTEROIDS \n",
    "readname = './data/all_asteroids_wrangled.csv'\n",
    "readname_comets = './data/all_comets_wrangled.csv'\n",
    "\n",
    "# ASTEROIDS >20KM in DIAMETER\n",
    "savename = './data/large_asteroids.csv'\n",
    "min_diameter, max_diameter = 20, np.inf\n",
    "min_distance, max_distance = 0, 240\n",
    "split_asteroids(readname, savename, min_distance, max_distance, \n",
    "                min_diameter, max_diameter, start_date, stop_date)\n",
    "\n",
    "# ASTEROIDS 10~20KM in DIAMETER\n",
    "savename = './data/small_asteroids.csv'\n",
    "min_diameter, max_diameter = 10, 20\n",
    "min_distance, max_distance = 0, 240\n",
    "split_asteroids(readname, savename, min_distance, max_distance, \n",
    "                min_diameter, max_diameter, start_date, stop_date)\n",
    "\n",
    "# COMETS >10KM in DIAMETER\n",
    "savename = './data/large_comets.csv'\n",
    "min_diameter, max_diameter = 10, np.inf\n",
    "min_distance, max_distance = 0, 240\n",
    "split_asteroids(readname_comets, savename, min_distance, max_distance, \n",
    "                min_diameter, max_diameter, start_date, stop_date)\n",
    "\n",
    "# ASTEROIDS\n",
    "savename = './data/any_outer_asteroids.csv'\n",
    "min_diameter, max_diameter = 'null', 'null'\n",
    "min_distance, max_distance = 3, 240\n",
    "split_asteroids(readname, savename, min_distance, max_distance, \n",
    "               min_diameter, max_diameter, start_date, stop_date, randomize=5000)\n",
    "\n",
    "# ASTEROIDS\n",
    "savename = './data/any_inner_asteroids.csv'\n",
    "min_diameter, max_diameter = 'null', 'null'\n",
    "min_distance, max_distance = 0, 2.5\n",
    "split_asteroids(readname, savename, min_distance, max_distance, \n",
    "                 min_diameter, max_diameter, start_date, stop_date, randomize=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "MBA    1734\n",
       "OMB     346\n",
       "GRK     296\n",
       "TJN     259\n",
       "TNO      41\n",
       "CEN      32\n",
       "MCA       4\n",
       "AMO       1\n",
       "AST       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "MBA    5191\n",
       "OMB    1035\n",
       "GRK     722\n",
       "TJN     495\n",
       "CEN       9\n",
       "MCA       5\n",
       "AST       4\n",
       "AMO       3\n",
       "IMB       2\n",
       "TNO       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "OMB    3034\n",
       "TNO    1736\n",
       "CEN     204\n",
       "AST      26\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "MCA    992\n",
       "IMB    920\n",
       "APO    543\n",
       "AMO    378\n",
       "OMB     89\n",
       "ATE     73\n",
       "CEN      3\n",
       "IEO      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "JFc    6\n",
       "HTC    4\n",
       "CTc    1\n",
       "COM    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output the different classes of asteroids and comets for reference\n",
    "\n",
    "ast = pd.read_csv('./data/large_asteroids.csv', low_memory=False)\n",
    "display(ast['class'].value_counts())\n",
    "\n",
    "ast = pd.read_csv('./data/small_asteroids.csv', low_memory=False)\n",
    "display(ast['class'].value_counts())\n",
    "\n",
    "ast = pd.read_csv('./data/any_outer_asteroids.csv', low_memory=False)\n",
    "display(ast['class'].value_counts())\n",
    "\n",
    "ast = pd.read_csv('./data/any_inner_asteroids.csv', low_memory=False)\n",
    "display(ast['class'].value_counts())\n",
    "\n",
    "com = pd.read_csv('./data/large_comets.csv', low_memory=False)\n",
    "display(com['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 GRKs\n"
     ]
    }
   ],
   "source": [
    "df_asts = pd.read_csv('./data/large_asteroids.csv')\n",
    "df_tjn = df_asts[df_asts['class'] == 'TJN'].copy()\n",
    "df_non_tjn = df_asts[df_asts['class'] != 'TJN'].copy()\n",
    "count, t = 0, 0\n",
    "indices = []\n",
    "\n",
    "for index, row in df_tjn.iterrows():\n",
    "    filename = \"./data/large_asteroids/\"+row['horizons']+\".csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        xs, ys = df[\"X\"].tolist(), df[\"Y\"].tolist()\n",
    "        theta = [get_angle(0, 0, x, y) for x, y in zip(xs, ys)]\n",
    "        theta = [np.radians(x) for x in theta]\n",
    "        if theta[-1] > 0.6294830920687847:\n",
    "            df_tjn.loc[index, 'class'] = 'GRK'\n",
    "            t += 1\n",
    "    except:\n",
    "        count += 1\n",
    "\n",
    "df = pd.concat([df_tjn, df_non_tjn])\n",
    "df.to_csv('./data/large_asteroids.csv', index=False)\n",
    "print(len(df[df['class'] == 'GRK']), 'GRKs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722 GRKs\n"
     ]
    }
   ],
   "source": [
    "df_asts = pd.read_csv('./data/small_asteroids.csv')\n",
    "df_tjn = df_asts[df_asts['class'] == 'TJN'].copy()\n",
    "df_non_tjn = df_asts[df_asts['class'] != 'TJN'].copy()\n",
    "count, t = 0, 0\n",
    "indices = []\n",
    "\n",
    "for index, row in df_tjn.iterrows():\n",
    "    filename = \"./data/small_asteroids/\"+row['horizons']+\".csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        xs, ys = df[\"X\"].tolist(), df[\"Y\"].tolist()\n",
    "        theta = [get_angle(0, 0, x, y) for x, y in zip(xs, ys)]\n",
    "        theta = [np.radians(x) for x in theta]\n",
    "        \n",
    "        # Angle of Jupiter position (found after HORIZONS search)\n",
    "        if theta[-1] > 0.6294830920687847:\n",
    "            df_tjn.loc[index, 'class'] = 'GRK'\n",
    "            t += 1\n",
    "    except:\n",
    "        count += 1\n",
    "\n",
    "df = pd.concat([df_tjn, df_non_tjn])\n",
    "df.to_csv('./data/small_asteroids.csv', index=False)\n",
    "print(len(df[df['class'] == 'GRK']), 'GRKs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
